{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctK9jTDGw3S4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "from tqdm import tqdm\n",
        "from sklearn.datasets import make_regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_regression(n_samples = 10000,\n",
        "                       n_features = 2,\n",
        "                       noise = 1,\n",
        "                       random_state = 42)"
      ],
      "metadata": {
        "id": "O_4EqHGGxDpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import det\n",
        "class Linear_Regression:\n",
        "\n",
        "  def __init__(self,X,y,tolerance=None,learning_rate=None,reg_param=None):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.tolerance = tolerance\n",
        "    self.learning_rate = learning_rate\n",
        "    self.reg_param = reg_param\n",
        "\n",
        "  def split_dataset(self):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(self.X,\n",
        "                                                            self.y,\n",
        "                                                            train_size=0.75,\n",
        "                                                            random_state=1,\n",
        "                                                            shuffle=True)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "  def normalize_train(self,X):\n",
        "    mean = np.mean(X,0)\n",
        "    std = np.std(X,0)\n",
        "    X = (X-mean)/std\n",
        "    return X, mean, std\n",
        "\n",
        "  def normalize_test(self,X,mean,std):\n",
        "    return (X-mean)/std\n",
        "\n",
        "  def addX0(self,X):\n",
        "    return np.column_stack([np.ones(X.shape[0]),X])\n",
        "\n",
        "  def closedformSol(self,X,y):\n",
        "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "  def predict(self,X):\n",
        "    return X.dot(self.w)\n",
        "\n",
        "  def lossfunction(self,X,y):\n",
        "    y_hat = self.predict(X)\n",
        "    return y - y_hat\n",
        "\n",
        "  def SSE(self,X,y):\n",
        "    y_hat = self.predict(X)\n",
        "    return ((y-y_hat)**2).sum()\n",
        "\n",
        "  def costfunction(self,X,y):\n",
        "    return self.SSE(X,y)/2 + self.reg_param*((self.closedformSol(X,y)**2).sum())\n",
        "\n",
        "  def costDerivative(self,X,y):\n",
        "    return X.T.dot(self.predict(X) - y)\n",
        "\n",
        "  def gradientDescent(self,X,y):\n",
        "    errors = []\n",
        "    last_err = float('inf')\n",
        "    self.w = np.ones(X.shape[1],dtype=np.float64)\n",
        "\n",
        "    for i in tqdm(range(X.shape[0])):\n",
        "      self.w = self.w - self.learning_rate * self.costDerivative(X,y)\n",
        "      current_err = self.SSE(X,y)\n",
        "      diff = abs(current_err - last_err)\n",
        "      last_err = current_err\n",
        "\n",
        "      if diff < self.tolerance:\n",
        "        print('model converged')\n",
        "        break\n",
        "\n",
        "      errors.append(diff)\n",
        "\n",
        "    print(self.w)\n",
        "\n",
        "  def regularization(self,X,y):\n",
        "    return self.costfunction(X,y) + self.reg_param * (self.w)\n",
        "\n",
        "  def fit(self):\n",
        "    self.split_dataset()\n",
        "    X_train, X_test, y_train, y_test = self.split_dataset()\n",
        "    X_train, mean, std = self.normalize_train(X_train)\n",
        "    X_test = self.normalize_test(X_test,mean,std)\n",
        "\n",
        "    self.gradientDescent(X_train,y_train)\n",
        "    print('coefficients are: {}'.format(self.w))"
      ],
      "metadata": {
        "id": "FIPxK66GxGDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = Linear_Regression(X,y,tolerance=0.00001,learning_rate=0.005)"
      ],
      "metadata": {
        "id": "gY7o129XxIrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.fit()"
      ],
      "metadata": {
        "id": "LU-7U4L4xN-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}